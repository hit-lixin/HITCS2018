# NLP预训练模型简单介绍



最近面试有不少人都问到了NLP中的预训练模型，在阅读原文之后加上查阅了一些资料之后，记录一下自己的了解，如有错漏请指出。





## 词向量

​		在介绍预训练模型之前，我先大致说一下单词是如何表示出来的 。2013年Google的Mikolov提出了跨时代的word2vec模型，利用上下文将单词表示为一个低维的连续实数向量，这个向量包含了单词的语义信息。

举个例子：

> **这个女孩子真（漂亮/美丽）**。

可以想到，漂亮和美丽出现的场景大多情况都是一样的，即上下文一样，因此在word2vec得到表示之后的两个词的向量也是接近的。

当然这里有许多问题，比如一词多意的问题，再举个例子：

>  **我今天去大姨妈家里玩了。** 
>
> **怎么还不来大姨妈！！**

这两句话中同样的一个词大姨妈的含义却是不一样的。那么怎么样才能知道每句话中同一个词的含义是否一样呢？这里就引出了*ELMO*（Embeddings from Language Models），这是一个deep contextualized word representation ，同时也是预训练模型的一个开端。

ps： word2vec也可以进行预训练，但是这里指的预训练均指的是大规模预训练，单单ELMO就有近一个亿的参数。

ps2: 关于一词多义的问题，清华大学的刘知远老师也有一个工作是利用意元来解决这个问题。他将词语的含义都进行了分解，也就是说复杂的含义都是由简单的含义（也就是意元）组成。而每一个词都对应着不同的意元，其在不同的情景下权重不相同。比如说苹果这个单词，一般具有两个意元，一个是水果，一个是苹果公司。在不同的上下文中，苹果的含义也不一样，因此引入了attention机制来对不同的意元进行attend。举个例子：

> **我喜欢用苹果手机。**

在这句话中，模型分别对两个意元进行attend，分别得到了0.1（水果）和0.9（苹果公司）。所以这个里面的苹果的表示更接近于苹果公司。

ref:http://nlp.csai.tsinghua.edu.cn/~lzy/publications/acl2017_sememe.pdf





## ELMO

​		EMLO是第一个使用大规模语料来训练语言模型（据我所知是这样的）的。这里插一句，语言模型指的就是针对任意一句话计算其出现的概率，而词向量基本都是语言模型的副产物，并且seq-seq问题多数情况下可以认为是一个条件语言模型。因此语言模型是NLP的基石，机器翻译、QA以及ASR都需要语言模型的支持。

这里我偷懒了直接引用了我的精神导师李宏毅（http://speech.ee.ntu.edu.tw/~tlkagk/）老师的PPT中的示意图

<img src="C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1567695065248.png" alt="1567695065248" style="zoom:80%;" />

简单介绍一下吧：

在ELMO原文中使用的bi-LSTM来进行建模，每一层的每一个单词都有一个前向（从前往后）和后向（从后往前）的隐状态，这两个隐状态拼接在一起得到了该单词这这一层的表示。如果这个LSTM是deep的，就把每一层的隐状态线性组合，如图：

> h `=` `a`\*h1 + b\*h2

这里的a和b作为超参数和后面的任务一起进行学习。







## BERT

自从2018年10月份bert横空出世以来，可谓已经到了街知巷闻的地步，以至于隔壁的CVer都会在吃饭的时候插一嘴聊两句。

说起bert，不得不说一下Transformer，后者是一个seq-seq模型的大杀器，由于大规模使用了自注意力机制而闻名。transformer的结构是下面这样的，想必都很熟悉，其相比较传统的LSTM主要有两方面的改进：参数减少，以及并行运算收敛很快。

![1567696293945](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1567696293945.png)

bert的结构呢就是类似于ELMO，只不过使用了transformer的encoder取代了LSTM。由于transformer的结构特性就决定了训练的时候不需要进行单个输入，因此想要训练语言模型就不能像ELMO那样了。为此bert的作者使用了一个独特的技巧，MLM（masked language model），以一定概率（0.15）使得句子中的任意一个单词被mask，然后使用其他的单词来预测被掩盖的单词（注：这里原文中引入 噪声，并不一定每次都是预测被mask的单词）。不仅如此，bert还引入了对句子的表示：

![1567697005224](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1567697005224.png)

每一次输入两个句子然后判断两个句子是否是上下文的关系。

在模型训练完成之后，会针对特有的任务再次进行微调，这个做法迁移学习中比较常用。这里我的介绍比较简陋，还是建议大家都去看一下原文，奇文共赏。





## ERNIE

写了这么多，其实还是为了引出ERNIE。ERNIE其实可以看做中文版的bert，主要的改进有两个：

第一个是引入了对话语言模型（DLM），对句子的上下文关系进行建模。

第二个是引入了知识图谱的概念，ERNIE中被mask的单词很多情况是短语或者实体，是为knowledge masking：

![1567697636358](C:\Users\lenovo\AppData\Roaming\Typora\typora-user-images\1567697636358.png)

这里就可以看出ERNIE这样做是可以让模型知道哈尔滨是黑龙江的省会的，并且是国际冰雪文化名城，这就自动引入了知识。

另外就是ERNIE的输入单位是字，这样做的好处就是省去了分词的烦恼和减少了输入数据的维数（中文的字远远少于词的数量）。

下面介绍一下ERNIE2.0，不得不说百度在技术上的积累不是盖的，相比较ERNIE，ERNIE2.0进行了更加精巧的设计。





## ERNIE2.0

TODO







## GPT2.0

TODO





## XLNET

TODO